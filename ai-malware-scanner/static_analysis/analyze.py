import os
import logging
import numpy as np
import lief
import hashlib
import magic
import re
from collections import Counter
# Change from relative import to absolute import
from static_analysis.entropy import shannon_entropy

# Initialize logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def get_imphash(binary):
    """Calculate import hash for PE file using LIEF."""
    try:
        imports = []
        for imported_library in binary.imports:
            library_name = imported_library.name.lower()
            for function in imported_library.entries:
                if function.name:
                    imports.append(f"{library_name}.{function.name}")
        imports = sorted(imports)
        import_str = ",".join(imports)
        return hashlib.md5(import_str.encode()).hexdigest()
    except Exception as e:
        logger.error(f"Error calculating imphash: {e}")
        return None

def extract_static_features(file_path):
    """Extract comprehensive static features from binary files using LIEF.
    Only process PE files, skip all other file types.
    """
    try:
        # Check if file exists and is accessible
        if not os.path.exists(file_path):
            logger.error(f"File does not exist: {file_path}")
            return None
            
        if not os.access(file_path, os.R_OK):
            logger.error(f"No permission to read file: {file_path}")
            return None
            
        # Basic file info
        file_size = os.path.getsize(file_path)
        
        # Handle very large files
        if file_size > 100 * 1024 * 1024:
            logger.warning(f"File is very large ({file_size/1024/1024:.2f} MB), may cause memory issues: {file_path}")
        
        # Use robust file type detection
        try:
            file_magic = magic.from_file(file_path)
            file_type = magic.from_file(file_path, mime=True)
        except Exception as e:
            logger.error(f"Error determining file type: {e}")
            file_magic = "unknown"
            file_type = "application/octet-stream"
        
        # Calculate hashes
        try:
            with open(file_path, "rb") as f:
                file_data = f.read()
                md5_hash = hashlib.md5(file_data).hexdigest()
                sha1_hash = hashlib.sha1(file_data).hexdigest()
                sha256_hash = hashlib.sha256(file_data).hexdigest()
        except Exception as e:
            logger.error(f"Error reading file or calculating hashes: {e}")
            return None
        
        # Skip analysis if file is not a PE file.
        if "PE32" not in file_magic and "PE32+" not in file_magic:
            logger.info(f"Skipping non-PE file: {file_path}")
            return None
        
        # Initialize features and mark as PE
        features = {
            "file_size": file_size,
            "file_type": file_type,
            "file_magic": file_magic,
            "md5": md5_hash,
            "sha1": sha1_hash,
            "sha256": sha256_hash,
            "entropy": shannon_entropy(file_data),
            "is_pe": True  # Confirmed PE from file_magic check
        }
        
        # PE specific features
        try:
            # Attempt to parse the PE file.
            try:
                binary = lief.parse(file_path)
            except Exception as parse_ex:
                if "DOS Header magic not found" in str(parse_ex):
                    logger.warning(f"DOS Header magic not found in file: {file_path}. Not a valid PE file.")
                    features["is_pe"] = False
                    features["lief_error"] = str(parse_ex)
                    return features
                else:
                    raise parse_ex
            
            if binary:
                features.update({
                    "imphash": get_imphash(binary),
                    "entry_point": binary.optional_header.addressof_entrypoint,
                    "image_base": binary.optional_header.imagebase,
                    "num_sections": len(binary.sections),
                    "num_directories": len(binary.data_directories),
                    "compile_time": binary.header.time_date_stamps,
                    "virtual_size": binary.optional_header.sizeof_image,
                })
                
                for attr_name, lief_attr in [
                    ("has_debug", "has_debug"),
                    ("has_tls", "has_tls"),
                    ("has_resources", "has_resources"),
                    ("has_relocations", "has_relocations"),
                ]:
                    features[attr_name] = getattr(binary, lief_attr, False)
                
                # Explicitly set digital signature status to False
                features["is_signed"] = False
                
                for attr_name, lief_attr in [
                    ("has_nx", "has_nx"),
                    ("has_no_seh", "has_no_seh"),
                ]:
                    features[attr_name] = getattr(binary.optional_header, lief_attr, False)
                
                # Import features
                imports = Counter()
                if binary.has_imports:
                    for imported_library in binary.imports:
                        for function in imported_library.entries:
                            if function.name:
                                imports[function.name] += 1
                features["num_imports"] = sum(imports.values())
                
                # Section features
                section_names = [section.name for section in binary.sections]
                section_entropies = [shannon_entropy(section.content) for section in binary.sections]
                section_sizes = [section.size for section in binary.sections]
                features["sections"] = section_names
                features["section_entropies"] = section_entropies
                features["section_sizes"] = section_sizes
                features["avg_section_entropy"] = np.mean(section_entropies) if section_entropies else 0
            else:
                logger.error(f"LIEF couldn't parse the PE file: {file_path}")
                features["is_pe"] = False
        except Exception as e:
            logger.error(f"Error analyzing PE file with LIEF: {str(e)}")
            features["is_pe"] = False
            features["lief_error"] = str(e)
        
        # Extract strings for the file
        try:
            strings_pattern = re.compile(b'[\x20-\x7f]{5,}')
            strings = strings_pattern.findall(file_data)
            features["strings"] = [s.decode(errors='ignore') for s in strings[:1000]]  # Limit to first 1000 strings
            features["num_strings"] = len(strings)
        except Exception as e:
            logger.error(f"Error extracting strings: {e}")
            features["strings"] = []
            features["num_strings"] = 0
        
        return features
    except Exception as e:
        logger.error(f"Unhandled error processing file {file_path}: {e}")
        return None

def vectorize_features(features):
    """Convert extracted features to numerical vector for model input.
    IMPORTANT: This vector must match the Ember dataset feature order.
    """
    if not features:
        logger.error("No features provided to vectorize")
        return None

    if not features.get("is_pe", False):
        logger.info("Not vectorizing non-PE file")
        return None

    try:
        # Build a more extensive feature vector for EMBER compatibility
        # The full EMBER feature set has 2381 features
        
        # Start with the basic features we have
        basic_features = [
            features.get("file_size", 0),                # maps to 'size'
            features.get("entropy", 0),                  # 'entropy'
            features.get("num_strings", 0),              # 'numstrings'
            0,                                           # 'byteentropy' (not extracted)
            0,                                           # 'histogram' (not extracted)
            0,                                           # 'avlength' (not extracted)
            0,                                           # 'printabledist' (not extracted)
            0,                                           # 'printables' (not extracted)
            features.get("entry_point", 0),              # 'entry'
            features.get("virtual_size", 0),             # 'vsize'
            int(features.get("has_debug", False)),       # 'has_debug'
            int(features.get("has_tls", False)),         # 'has_tls'
            int(features.get("has_resources", False)),   # 'has_resources'
            int(features.get("has_relocations", False)), # 'has_relocations'
            int(features.get("has_signature", False)),   # 'has_signature'
            features.get("num_imports", 0)               # 'imports_counts'
        ]
        
        # Create padding to match EMBER feature vector size
        # EMBER has 2381 features total, we have 16 so far
        padding = [0] * (2381 - len(basic_features))
        
        # Combine basic features with padding
        feature_vector = basic_features + padding
        
        return feature_vector
    except Exception as e:
        logger.error(f"Error vectorizing features: {e}")
        return None
